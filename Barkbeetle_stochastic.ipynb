{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = {\"t1\", \"t2\", \"t3\"}\n",
    "\n",
    "for name in list(globals().keys()):\n",
    "    if name not in keep and not name.startswith(\"_\"):\n",
    "        del globals()[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.21 s, sys: 241 ms, total: 1.45 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#DATA IMPORTATION -- and general model formulation\n",
    "from __future__ import division\n",
    "from pyomo.environ import *\n",
    "\n",
    "from pyomo.opt import SolverStatus, TerminationCondition\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy\n",
    "import pickle\n",
    "import random\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "path_data = \"/projappl/project_2011004/BarkBeetle/\"\n",
    "path_out = \"/projappl/project_2011004/BarkBeetle/opt_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "class optimization_stochastic:\n",
    "    def __init__(self):\n",
    "        c = 0\n",
    "        self.area = 901.238\n",
    "        self.data1 = pd.read_parquet(path_data + 'barkbeetle_data_v2.parquet', engine='pyarrow')\n",
    "        self.data_orig = pd.read_parquet(path_data + 'barkbeetle_data_v2.parquet', engine='pyarrow')\n",
    "        self.data_orig = self.data_orig.set_index(['stand','schedule','sample','period'])\n",
    "        #To reduce the data a bit\n",
    "        self.combinations = 100\n",
    "        self.data1 = self.data1[self.data1['sample']<=self.combinations]\n",
    "        self.data = self.data1\n",
    "        \n",
    "        data1 = self.data.set_index(['stand','schedule','sample','period'])\n",
    "        stands = list(data1.loc[slice(None),slice(None),0,1].index.get_level_values(0))\n",
    "        self.data = self.data[self.data['stand'].isin(stands)]\n",
    "\n",
    "\n",
    "        self.data_opt = self.data\n",
    "\n",
    "        self.all_data = self.data_opt\n",
    "\n",
    "        self.Index_values = self.all_data.set_index(['stand','schedule']).index.unique()\n",
    "        self.Index_sample_year = self.all_data.set_index(['sample','period']).index.unique()\n",
    "        self.all_data = self.all_data.set_index(['stand','schedule','sample','period'])\n",
    "        \n",
    "        self.all_data = self.all_data.fillna(0)\n",
    "        \n",
    "        self.createModel()\n",
    "        self.ADD_OBJECTIVE_FUNCTION()\n",
    "\n",
    "    def ADD_OBJECTIVE_FUNCTION(self):\n",
    "        self.model1.NPV= Var(within=NonNegativeReals)\n",
    "        def NPV_INVENTORY(model1):\n",
    "            row_sum = sum(sum( (self.all_data.npv.loc[(s,r,sam,12.0)]*self.model1.X1[(s,r)]*self.area)  for (s,r) in self.model1.index1) for sam in range(0,self.combinations))/self.combinations\n",
    "            return self.model1.NPV ==row_sum\n",
    "        self.model1.NPV_INV= Constraint(rule=NPV_INVENTORY)\n",
    "\n",
    "        self.model1.Periodic_volume= Var(self.model1.year,self.model1.sample,within=NonNegativeReals)\n",
    "        def PERIOD_INVENTORY(model1,p,sam):\n",
    "            row_sum = sum( (self.all_data.vol_t.loc[(s,r,sam,p)]*self.model1.X1[(s,r)]*self.area)  for (s,r) in self.model1.index1)\n",
    "            return self.model1.Periodic_volume[p,sam] ==row_sum\n",
    "        self.model1.Period_INV= Constraint(self.model1.year,self.model1.sample,rule=PERIOD_INVENTORY)\n",
    "\n",
    "        # A deterministic approach to ensure even flow -- useful to calculate the maximium even flow (a max the minimum problem)\n",
    "        self.model1.DEC_inc = Var(self.model1.sample,within=NonNegativeReals)\n",
    "        # INC constraints\n",
    "        def inc_bounded_rule(model1, sam,p):\n",
    "            INC2 = sum(\n",
    "                (self.all_data.vol_t.loc[(s,r,sam,p)] ) * self.model1.X1[(s, r)] *self.area\n",
    "                for (s,r) in self.model1.index1)\n",
    "            return INC2 >= self.model1.DEC_inc[sam]\n",
    "\n",
    "        self.model1.INC_bounded = Constraint(self.Index_sample_year, rule=inc_bounded_rule)\n",
    "\n",
    "\n",
    "        def outcome_rule(model1):\n",
    "            NPV = self.model1.NPV\n",
    "            return NPV\n",
    "        self.model1.OBJ = Objective(rule=outcome_rule, sense=maximize)\n",
    "\n",
    "        #CVAR Part:\n",
    "        ##CVAR: construct CVAR model for stochastic model\n",
    "\n",
    "        #Downside CVaR\n",
    "        self.model1.CVAR_down = Var(self.model1.year,within=Reals, initialize=1)\n",
    "        self.model1.VAR_down = Var(self.model1.year,within=Reals, initialize=1)\n",
    "        self.model1.posVAR_down = Var(self.model1.year,self.model1.sample, within=NonNegativeReals, initialize=1)\n",
    "        self.model1.negVAR_down = Var(self.model1.year,self.model1.sample, within=NonNegativeReals, initialize=1)\n",
    "        self.model1.mod_L_plus_down = Var(self.model1.year,self.model1.sample, within=NonNegativeReals, initialize=1)\n",
    "        self.model1.mod_L_neg_down = Var(self.model1.year,self.model1.sample, within=NonNegativeReals, initialize=1)\n",
    "        self.model1.alpha_risk_down = Param(default=0.80, mutable=True)\n",
    "        self.model1.min_CVaR_down = Param(default=0, mutable=True)\n",
    "        self.model1.max_CVaR_down = Param(default=1, mutable=True)\n",
    "        self.model1.target_down = Param(default=3000, mutable=True)\n",
    "        self.model1.CVAR_down_Tot = Var(within=Reals)\n",
    "        self.model1.samples = len(self.model1.sample)\n",
    "\n",
    "        #Downside CVaR Constraints LOG\n",
    "        def CVAR_constraint_down(model1,k):\n",
    "            CVAR_down = self.model1.VAR_down[k] + (1/((1-self.model1.alpha_risk_down)*self.model1.samples))*sum(self.model1.posVAR_down[k,sample] for sample in self.model1.sample)\n",
    "            return self.model1.CVAR_down[k] == CVAR_down\n",
    "        self.model1.CVAR_constraint_down = Constraint(self.model1.year,rule=CVAR_constraint_down)\n",
    "\n",
    "        def MOD_L_P_constraint_down(model1,k,it):\n",
    "            VAL_down = self.model1.mod_L_plus_down[k,it] -self.model1.VAR_down[k]+self.model1.negVAR_down[k,it]-self.model1.posVAR_down[k,it]\n",
    "            return VAL_down == 0\n",
    "        self.model1.MOD_L_P_down = Constraint(self.model1.year,self.model1.sample,rule=MOD_L_P_constraint_down)\n",
    "\n",
    "        def MOD_TARGET_constraint_down(model1,k,it):\n",
    "            row_sum = sum(((self.all_data.vol_t.loc[(s,r,it,k)])* self.model1.X1[(s,r)]*self.area) for (s,r) in self.model1.index1)\n",
    "            VAL = self.model1.target_down-row_sum - self.model1.mod_L_plus_down[k,it]+self.model1.mod_L_neg_down[k,it]\n",
    "            return VAL == 0\n",
    "        self.model1.MOD_Target_down = Constraint(self.model1.year,self.model1.sample,rule=MOD_TARGET_constraint_down)\n",
    "\n",
    "        def CVAR_down_constraint(model1):\n",
    "            row_sum = sum(self.model1.CVAR_down[k] for k in self.model1.year)\n",
    "            return self.model1.CVAR_down_Tot == row_sum \n",
    "        self.model1.CVAR_tot_down = Constraint(rule=CVAR_down_constraint)\n",
    "\n",
    "                                    \n",
    "    def createModel(self):\n",
    "        # Declare sets - These used to recongnize the number of stands, regimes and number of periods in the analysis.\n",
    "        self.model1 = ConcreteModel()\n",
    "        \n",
    "        self.model1.stands = Set(initialize = list(set(self.all_data.index.get_level_values(0))))\n",
    "        self.model1.year = Set(initialize = list(set(self.all_data.index.get_level_values(3))))\n",
    "        self.model1.regimes = Set(initialize = list(set(self.all_data.index.get_level_values(1))))\n",
    "        self.model1.sample = Set(initialize = list(set(self.all_data.index.get_level_values(2))))\n",
    "        self.model1.Index_values = self.Index_values\n",
    "\n",
    "        \n",
    "        def index_rule(model1):\n",
    "            index = []\n",
    "            for (s,r) in model1.Index_values: #stand_set\n",
    "                index.append((s,r))\n",
    "            return index            \n",
    "        self.model1.index1 = Set(dimen=2, initialize=index_rule)\n",
    "                \n",
    "        self.model1.X1 = Var(self.model1.index1, within=NonNegativeReals)\n",
    "        \n",
    "        self.all_data['period'] = self.all_data.index.get_level_values(2)\n",
    "        \n",
    "        def regime_rule(model1, s):\n",
    "            row_sum = sum(model1.X1[(s,r)] for r in [x[1] for x in model1.index1 if x[0] == s])\n",
    "            return row_sum == 1\n",
    "        self.model1.regime_limit = Constraint(self.model1.stands, rule=regime_rule)\n",
    "        \n",
    "    def solve(self):\n",
    "        opt = SolverFactory('cbc') #Here we use the cplex, other solvers such as cbc solver possible -- open source software\n",
    "        self.results = opt.solve(self.model1,tee=True) #We solve a problem, but do not show the solver output\n",
    "        \n",
    "\n",
    "t1 = optimization_stochastic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "class optimization_deterministic:\n",
    "    def __init__(self):\n",
    "        c = 0\n",
    "            self.data1 = pd.read_parquet(path_data + 'barkbeetle_deterministic.parquet', engine='pyarrow')\n",
    "        self.area = 901.238\n",
    "        #This is here to keep the dataframes seem similar. Could adjust.\n",
    "        self.combinations = 1\n",
    "        self.data1 = self.data1[self.data1['sample']<=self.combinations]\n",
    "        self.data = self.data1\n",
    "        \n",
    "        data1 = self.data.set_index(['stand','schedule','sample','period'])\n",
    "        stands = list(data1.loc[slice(None),slice(None),0,1].index.get_level_values(0))\n",
    "        self.data = self.data[self.data['stand'].isin(stands)]\n",
    "\n",
    "        self.data_opt = self.data\n",
    "\n",
    "        self.all_data = self.data_opt\n",
    "\n",
    "        self.Index_values = self.all_data.set_index(['stand','schedule']).index.unique()\n",
    "        self.Index_sample_year = self.all_data.set_index(['sample','period']).index.unique()\n",
    "        self.all_data = self.all_data.set_index(['stand','schedule','sample','period'])\n",
    "        \n",
    "        self.all_data = self.all_data.fillna(0)\n",
    "        \n",
    "        self.createModel()\n",
    "        self.ADD_OBJECTIVE_FUNCTION()\n",
    "\n",
    "    def ADD_OBJECTIVE_FUNCTION(self):\n",
    "        self.model1.NPV= Var(within=NonNegativeReals)\n",
    "        def NPV_INVENTORY(model1):\n",
    "            row_sum = sum(sum( (self.all_data.npv.loc[(s,r,sam,12.0)]*self.model1.X1[(s,r)]*self.area)  for (s,r) in self.model1.index1) for sam in range(0,self.combinations))/self.combinations\n",
    "            return self.model1.NPV ==row_sum\n",
    "        self.model1.NPV_INV= Constraint(rule=NPV_INVENTORY)\n",
    "\n",
    "        self.model1.Periodic_volume= Var(self.model1.year,self.model1.sample,within=NonNegativeReals)\n",
    "        def PERIOD_INVENTORY(model1,p,sam):\n",
    "            row_sum = sum( (self.all_data.vol_t.loc[(s,r,sam,p)]*self.model1.X1[(s,r)]*self.area)  for (s,r) in self.model1.index1)\n",
    "            return self.model1.Periodic_volume[p,sam] ==row_sum\n",
    "        self.model1.Period_INV= Constraint(self.model1.year,self.model1.sample,rule=PERIOD_INVENTORY)\n",
    "\n",
    "      \n",
    "        self.model1.DEC_inc = Var(self.model1.sample,within=NonNegativeReals)\n",
    "        # INC constraints\n",
    "        def inc_bounded_rule(model1, sam,p):\n",
    "            INC2 = sum(\n",
    "                (self.all_data.vol_t.loc[(s,r,sam,p)] ) * self.model1.X1[(s, r)] *self.area\n",
    "                for (s,r) in self.model1.index1)\n",
    "            return INC2 >= self.model1.DEC_inc[sam]\n",
    "\n",
    "        self.model1.INC_bounded = Constraint(self.Index_sample_year, rule=inc_bounded_rule)\n",
    "\n",
    "        self.model1.DEC_inc_limit = Param(default = 3000,mutable = True)\n",
    "        # INC constraints\n",
    "        def inc_bounded_limit_rule(model1, sam,p):\n",
    "            INC2 = sum(\n",
    "                (self.all_data.vol_t.loc[(s,r,sam,p)] ) * self.model1.X1[(s, r)] *self.area\n",
    "                for (s,r) in self.model1.index1)\n",
    "            return INC2 >= self.model1.DEC_inc_limit\n",
    "        \n",
    "        self.model1.INC_bounded_limit = Constraint(self.Index_sample_year, rule=inc_bounded_limit_rule)\n",
    "\n",
    "\n",
    "        def outcome_rule(model1):\n",
    "            NPV = self.model1.NPV\n",
    "            return NPV\n",
    "        self.model1.OBJ = Objective(rule=outcome_rule, sense=maximize)\n",
    "                                    \n",
    "    def createModel(self):\n",
    "        # Declare sets - These used to recongnize the number of stands, regimes and number of periods in the analysis.\n",
    "        self.model1 = ConcreteModel()\n",
    "        \n",
    "        self.model1.stands = Set(initialize = list(set(self.all_data.index.get_level_values(0))))\n",
    "        self.model1.year = Set(initialize = list(set(self.all_data.index.get_level_values(3))))\n",
    "        self.model1.regimes = Set(initialize = list(set(self.all_data.index.get_level_values(1))))\n",
    "        self.model1.sample = Set(initialize = list(set(self.all_data.index.get_level_values(2))))\n",
    "        self.model1.Index_values = self.Index_values\n",
    "\n",
    "        \n",
    "        def index_rule(model1):\n",
    "            index = []\n",
    "            for (s,r) in model1.Index_values: #stand_set\n",
    "                index.append((s,r))\n",
    "            return index            \n",
    "        self.model1.index1 = Set(dimen=2, initialize=index_rule)\n",
    "        \n",
    "        self.model1.X1 = Var(self.model1.index1, within=NonNegativeReals)\n",
    "        \n",
    "        self.all_data['period'] = self.all_data.index.get_level_values(2)\n",
    "        \n",
    "        #objective function: Can add if needednew perspective is \n",
    "        \n",
    "        def regime_rule(model1, s):\n",
    "            row_sum = sum(model1.X1[(s,r)] for r in [x[1] for x in model1.index1 if x[0] == s])\n",
    "            return row_sum == 1\n",
    "        self.model1.regime_limit = Constraint(self.model1.stands, rule=regime_rule)\n",
    "        \n",
    "    def solve(self):\n",
    "        opt = SolverFactory('cbc') #Here we use the cplex solver, other solvers such as cbc solver possible -- open source software\n",
    "        self.results = opt.solve(self.model1,tee=True) #We solve a problem, but do not show the solver output\n",
    "\n",
    "t2 = optimization_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "class optimization_robust:\n",
    "    def __init__(self):\n",
    "        c = 0\n",
    "        self.data1 = pd.read_parquet(path_data + 'barkbeetle_deterministic.parquet', engine='pyarrow')\n",
    "        self.area = 901.238\n",
    "        #This is to keep the dataframes seem similar. Could adjust.\n",
    "        self.combinations = 1\n",
    "        self.data1 = self.data1[self.data1['sample']<=self.combinations]\n",
    "        self.data = self.data1\n",
    "        \n",
    "        data1 = self.data.set_index(['stand','schedule','sample','period'])\n",
    "        stands = list(data1.loc[slice(None),slice(None),0,1].index.get_level_values(0))\n",
    "        self.data = self.data[self.data['stand'].isin(stands)]\n",
    "\n",
    "        self.data_opt = self.data\n",
    "\n",
    "        self.all_data = self.data_opt\n",
    "\n",
    "        self.Index_values = self.all_data.set_index(['stand','schedule']).index.unique()\n",
    "        self.Index_sample_year = self.all_data.set_index(['sample','period']).index.unique()\n",
    "        self.all_data = self.all_data.set_index(['stand','schedule','sample','period'])\n",
    "        \n",
    "        self.all_data = self.all_data.fillna(0)\n",
    "        \n",
    "        self.createModel()\n",
    "        self.ADD_OBJECTIVE_FUNCTION()\n",
    "\n",
    "    def ADD_OBJECTIVE_FUNCTION(self):\n",
    "        self.model1.NPV= Var(within=NonNegativeReals)\n",
    "        def NPV_INVENTORY(model1):\n",
    "            row_sum = sum(sum( (self.all_data.npv.loc[(s,r,sam,12.0)]*self.model1.X1[(s,r)]*self.area)  for (s,r) in self.model1.index1) for sam in range(0,self.combinations))/self.combinations\n",
    "            return self.model1.NPV ==row_sum\n",
    "        self.model1.NPV_INV= Constraint(rule=NPV_INVENTORY)\n",
    "\n",
    "        self.model1.Periodic_volume= Var(self.model1.year,self.model1.sample,within=NonNegativeReals)\n",
    "        def PERIOD_INVENTORY(model1,p,sam):\n",
    "            row_sum = sum( (self.all_data.vol_t.loc[(s,r,sam,p)]*self.model1.X1[(s,r)]*self.area)  for (s,r) in self.model1.index1)\n",
    "            return self.model1.Periodic_volume[p,sam] ==row_sum\n",
    "        self.model1.Period_INV= Constraint(self.model1.year,self.model1.sample,rule=PERIOD_INVENTORY)\n",
    "\n",
    "        self.model1.Gamma_t = Param(self.model1.year,default = 0.5, mutable =True)\n",
    "        \n",
    "        self.model1.u_t = Var(self.model1.year,within= NonNegativeReals)\n",
    "        self.model1.w_srt = Var(self.model1.index1,self.model1.year,within= NonNegativeReals)\n",
    "      \n",
    "        self.model1.DEC_inc = Var(self.model1.sample,within=NonNegativeReals)\n",
    "        self.model1.DEC_inc_robust = Param(default=3000,mutable = True)\n",
    "        \n",
    "        # INC constraints - From deterministic version.\n",
    "        #def inc_bounded_rule(model1, sam,p):\n",
    "        #    INC2 = sum(\n",
    "        #        (self.all_data.vol_t.loc[(s,r,sam,p)] ) * self.model1.X1[(s, r)] \n",
    "        #        for (s,r) in self.model1.index1)\n",
    "        #    return INC2 >= self.model1.DEC_inc[sam]\n",
    "        #self.model1.INC_bounded = Constraint(self.Index_sample_year, rule=inc_bounded_rule)\n",
    "\n",
    "        # Gamma INC constraints\n",
    "        def inc_bounded_robust_rule(model1, sam,p):\n",
    "            INC2 = sum((self.all_data.vol_t.loc[(s,r,sam,p)] ) * self.model1.X1[(s, r)]*self.area for (s,r) in self.model1.index1) - (\n",
    "                self.model1.Gamma_t[p]*self.model1.u_t[p]) - (sum(self.model1.w_srt[s,r,p] for (s,r) in self.model1.index1))\n",
    "            return INC2 >= self.model1.DEC_inc_robust\n",
    "        self.model1.INC_bounded = Constraint(self.Index_sample_year, rule=inc_bounded_robust_rule)\n",
    "\n",
    "        #Additional robust constraints #Assuming 2% volume damage from bark beetle. Could be more specific\n",
    "        def robust_rule_1(model1, sam,p,s,r):\n",
    "            INC_R = 0.02*((self.all_data.vol_t.loc[(s,r,sam,p)] ) * self.model1.X1[(s, r)]*self.area)\n",
    "            return self.model1.u_t[p]+self.model1.w_srt[s,r,p] >=INC_R\n",
    "        self.model1.Robust_rule_one= Constraint(self.Index_sample_year, self.model1.index1, rule=robust_rule_1)\n",
    "\n",
    "        def outcome_rule(model1):\n",
    "            NPV = self.model1.NPV\n",
    "            return NPV\n",
    "        self.model1.OBJ = Objective(rule=outcome_rule, sense=maximize)\n",
    "                                    \n",
    "    def createModel(self):\n",
    "        # Declare sets - These used to recongnize the number of stands, regimes and number of periods in the analysis.\n",
    "        self.model1 = ConcreteModel()\n",
    "        \n",
    "        self.model1.stands = Set(initialize = list(set(self.all_data.index.get_level_values(0))))\n",
    "        self.model1.year = Set(initialize = list(set(self.all_data.index.get_level_values(3))))\n",
    "        self.model1.regimes = Set(initialize = list(set(self.all_data.index.get_level_values(1))))\n",
    "        self.model1.sample = Set(initialize = list(set(self.all_data.index.get_level_values(2))))\n",
    "        self.model1.Index_values = self.Index_values\n",
    "\n",
    "        \n",
    "        def index_rule(model1):\n",
    "            index = []\n",
    "            for (s,r) in model1.Index_values: #stand_set\n",
    "                index.append((s,r))\n",
    "            return index            \n",
    "        self.model1.index1 = Set(dimen=2, initialize=index_rule)\n",
    "        \n",
    "        self.model1.X1 = Var(self.model1.index1, within=NonNegativeReals)\n",
    "        \n",
    "        self.all_data['period'] = self.all_data.index.get_level_values(2)\n",
    "        \n",
    "        #objective function: Can add if needednew perspective is \n",
    "        \n",
    "        def regime_rule(model1, s):\n",
    "            row_sum = sum(model1.X1[(s,r)] for r in [x[1] for x in model1.index1 if x[0] == s])\n",
    "            return row_sum == 1\n",
    "        self.model1.regime_limit = Constraint(self.model1.stands, rule=regime_rule)\n",
    "        \n",
    "    def solve(self):\n",
    "        opt = SolverFactory('cbc') #Here we use the cplex solver, other solvers such as cbc solver possible -- open source software\n",
    "        self.results = opt.solve(self.model1,tee=True) #We solve a problem, but do not show the solver output\n",
    "\n",
    "t3 = optimization_robust()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Maximize only for NPV: \n",
    "#Minmimize deviations -- 3 approaches\n",
    "#Stochastic:\n",
    "TYPE = \"MAX_NPV\"\n",
    "t1.model1.target_down = 0\n",
    "def outcome_rule(model1):\n",
    "    NPV = t1.model1.NPV-t1.model1.CVAR_down_Tot *1000\n",
    "    return NPV\n",
    "t1.model1.OBJ = Objective(rule=outcome_rule, sense=maximize)\n",
    "t1.solve()\n",
    "\n",
    "#Deterministic\n",
    "t2.model1.DEC_inc_limit = 0\n",
    "t2.solve()\n",
    "\n",
    "\n",
    "#Robust\n",
    "t3.model1.DEC_inc_robust = 0\n",
    "\n",
    "for p in t3.model1.year:\n",
    "    t3.model1.Gamma_t[p] = 150# Need to consider how to select the gamma parameter. Could reflect on Palma and nelson.\n",
    "t3.solve()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#Calculation of core values:\n",
    "#Resampling to ensure anticipatory planning doesn't occur.\n",
    "\n",
    "# Optimized function to generate random values for sample\n",
    "def sample_random(size):\n",
    "    return np.random.choice(range(100), size=size, replace=False)\n",
    "\n",
    "# Optimized version of create_sample\n",
    "def create_sample(df):\n",
    "    df_reset = df.reset_index()\n",
    "    unique_stands = df_reset['stand'].unique()\n",
    "    \n",
    "    # Loop through each stand and replace the sample values\n",
    "    for stand in unique_stands:\n",
    "        # Create a mask for the current stand\n",
    "        mask = df_reset['stand'] == stand\n",
    "        \n",
    "        # Get the unique sample values for this stand\n",
    "        unique_samples = df_reset.loc[mask, 'sample'].unique()\n",
    "        \n",
    "        # Generate random values for the current stand's sample values\n",
    "        #random_mapping = dict(zip(unique_samples, sample_random(100)))#len(unique_samples))))\n",
    "        \n",
    "        df_x = df_reset[df_reset['stand'] ==stand]#, 'sample'] = df_reset4.loc[mask, 'sample'].map(random_mapping)\n",
    "        sample=sample_random(100)\n",
    "        df_x =df_x[df_x['sample'].isin(sample)]\n",
    "        order = {s: i for i, s in enumerate(sample)}\n",
    "\n",
    "        # sort df_x according to sample order\n",
    "        df_x = df_x.sort_values(\n",
    "            by='sample',\n",
    "            key=lambda s: s.map(order)\n",
    "        )\n",
    "        \n",
    "        df_x['sample'] = pd.factorize(df_x['sample'])[0]\n",
    "        \n",
    "        if stand == unique_stands[0]:\n",
    "            df_y = df_x\n",
    "        else:\n",
    "            df_y = pd.concat([df_y,df_x])\n",
    "    return df_y\n",
    "\n",
    "# Constructing a sample:\n",
    "df_reset = create_sample(df=t1.data_orig)\n",
    "\n",
    "df_reset = df_reset[df_reset['sample']!=-1]\n",
    "df_reset = df_reset.set_index(['stand', 'schedule','sample', 'period'])\n",
    "\n",
    "# Pre-fetching all necessary data to avoid repeated lookups\n",
    "npv_data = df_reset.npv\n",
    "npv_data_det = t2.all_data.npv\n",
    "\n",
    "VOL_t_data = df_reset.vol_t\n",
    "VOL_t_data_det = t2.all_data.vol_t\n",
    "\n",
    "# Convert X1_values to a Pandas DataFrame\n",
    "#T1 -- is the stochastic solution\n",
    "T1_values_dict = {index: t1.model1.X1[index].value for index in t1.model1.X1}\n",
    "T1_values_df = pd.DataFrame.from_dict(T1_values_dict, orient='index', columns=['T1_value'])\n",
    "T1_values_df.index = pd.MultiIndex.from_tuples(T1_values_df.index)  # Convert index to MultiIndex\n",
    "\n",
    "#T2 -- is the deterministic solution\n",
    "T2_values_dict = {index: t2.model1.X1[index].value for index in t2.model1.X1}\n",
    "T2_values_df = pd.DataFrame.from_dict(T2_values_dict, orient='index', columns=['T2_value'])\n",
    "T2_values_df.index = pd.MultiIndex.from_tuples(T2_values_df.index)  # Convert index to MultiIndex if (s, r) is a tuple\n",
    "\n",
    "#T3 -- is the robust solution\n",
    "T3_values_dict = {index: t3.model1.X1[index].value for index in t3.model1.X1}\n",
    "T3_values_df = pd.DataFrame.from_dict(T3_values_dict, orient='index', columns=['T3_value'])\n",
    "T3_values_df.index = pd.MultiIndex.from_tuples(T3_values_df.index)  # Convert index to MultiIndex if (s, r) is a tuple\n",
    "\n",
    "T1_values_df = T1_values_df.copy()\n",
    "T2_values_df = T2_values_df.copy()\n",
    "T3_values_df = T3_values_df.copy()\n",
    "\n",
    "T1_values_df.index = T1_values_df.index.set_names(['stand', 'schedule'])\n",
    "T2_values_df.index = T2_values_df.index.set_names(['stand', 'schedule'])\n",
    "T3_values_df.index = T3_values_df.index.set_names(['stand', 'schedule'])\n",
    "\n",
    "npv_sto,npv_det, npv_rob = [],[],[]\n",
    "ef_sto, ef_det, ef_rob = {},{},{}\n",
    "\n",
    "for i in range(0, 100):\n",
    "    npv_sto =npv_sto+[(float(T1_values_df.mul(npv_data.loc[slice(None), slice(None), i, 1], axis=0).sum(axis=0).item()))*901.238]\n",
    "    npv_det =npv_det+[(float(T2_values_df.mul(npv_data.loc[slice(None), slice(None), i, 1], axis=0).sum(axis=0).item()))*901.238]\n",
    "    npv_rob =npv_rob+[(float(T3_values_df.mul(npv_data.loc[slice(None), slice(None), i, 1], axis=0).sum(axis=0).item()))*901.238]\n",
    "\n",
    "for k in range(1,13):\n",
    "    ef_sto[k] = [float(T1_values_df.mul(VOL_t_data.loc[slice(None), slice(None), i, k], axis=0).sum(axis=0).item())*901.238   for i in range(0, 100)]\n",
    "    ef_det[k] = [float(T2_values_df.mul(VOL_t_data.loc[slice(None), slice(None), i, k], axis=0).sum(axis=0).item())*901.238   for i in range(0, 100)]\n",
    "    ef_rob[k] = [float(T3_values_df.mul(VOL_t_data.loc[slice(None), slice(None), i, k], axis=0).sum(axis=0).item())*901.238   for i in range(0, 100)]\n",
    "    \n",
    "print(\"Stochastic\",(float(T1_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item())))\n",
    "print(\"Deterministic\", (float(T2_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item())))\n",
    "print(\"Robust\", (float(T3_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item())))\n",
    "\n",
    "print(\"Perceived decline in stochastic solution:\", (float(T1_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item()))/(float(T2_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item())))\n",
    "\n",
    "print(\"Stochastic\",(np.mean([npv_sto])))\n",
    "print(\"Deterministic\", (np.mean([npv_det])))\n",
    "print(\"Robust\", (np.mean([npv_rob])))\n",
    "print(\"Stochastic solution improvement:\", (np.mean([npv_sto]))/(np.mean([npv_det])))\n",
    "print(\"Stochastic solution improvement:\", (np.mean([npv_sto]))/(np.mean([npv_rob])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "##NPV ANALYSIS -- constructing the figures that present the distribution of the NPV and the scenario wise comparison\n",
    "\n",
    "sto_color = \"#0072B2\"  # blue\n",
    "rob_color = \"#D55E00\"  # vermillion\n",
    "harvest_codes = {18,19,20,21,22,23,24,25,26}\n",
    "\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, len(x) + 1) / len(x)\n",
    "    return x, y\n",
    "\n",
    "# scenario-wise NPVs\n",
    "npv_detx = np.array(npv_det)\n",
    "npv_stox = np.array(npv_sto)\n",
    "npv_robx = np.array(npv_rob)\n",
    "\n",
    "diff_sorted = np.sort(npv_stox - npv_detx)\n",
    "diff_sorted_rob = np.sort(npv_stox - npv_robx)\n",
    "\n",
    "colors = ['tab:blue' if d > 0 else 'tab:red' for d in diff_sorted]\n",
    "\n",
    "\n",
    "def lighten(color, amount=0.5):\n",
    "    c = mcolors.to_rgb(color)\n",
    "    return tuple(1 - amount*(1 - x) for x in c)\n",
    "\n",
    "light_blue = lighten('tab:blue', 0.4)\n",
    "light_red  = lighten('tab:red', 0.4)\n",
    "\n",
    "colors_rob = [light_blue if d > 0 else light_red for d in diff_sorted_rob]\n",
    "\n",
    "# scenario-wise NPVs\n",
    "npv_detx = np.array(npv_det)\n",
    "npv_stox = np.array(npv_sto)\n",
    "npv_robx = np.array(npv_rob)\n",
    "\n",
    "# ---- figure with two panels ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), constrained_layout=True)\n",
    "\n",
    "# --- (a) Distribution (hist + optional KDE) ---\n",
    "ax = axes[0]\n",
    "# Define one base color per scenario\n",
    "colorsX = {    \"Stochastic\": \"#1f77b4\",   \"Deterministic\": \"#d62728\",    \"Robust\": \"#2ca02c\",\"BOTH\": \"red\"}\n",
    "\n",
    "all_vals = np.concatenate([npv_stox, npv_robx, npv_detx])\n",
    "bins = np.histogram_bin_edges(all_vals, bins=\"fd\")\n",
    "bins = 50\n",
    "# Histograms (light + transparent)\n",
    "ax.hist(npv_stox, bins=bins, density=True,  color=colorsX[\"Stochastic\"], alpha=0.30, label=\"Stochastic\")\n",
    "if TYPE != \"MAX_NPV\":\n",
    "    ax.hist(npv_detx, bins=bins, density=True, color=colorsX[\"Deterministic\"], alpha=0.30, label=\"Deterministic\")\n",
    "    ax.hist(npv_robx, bins=bins, density=True, color=colorsX[\"Robust\"], alpha=0.30, label=\"Robust\")\n",
    "else:\n",
    "    ax.hist(npv_robx, bins=bins, density=True, color=colorsX[\"BOTH\"], alpha=0.30, label=\"Deterministic and Robust\")\n",
    "# KDE lines (same color, darker + thicker)\n",
    "xs = np.linspace(all_vals.min(), all_vals.max(), 300)\n",
    "\n",
    "ax.plot(xs, gaussian_kde(npv_stox)(xs), color=colorsX[\"Stochastic\"], linewidth=2.5)\n",
    "if TYPE != \"MAX_NPV\":\n",
    "    ax.plot(xs, gaussian_kde(npv_detx)(xs), color=colorsX[\"Deterministic\"], linewidth=2.5)\n",
    "    ax.plot(xs, gaussian_kde(npv_robx)(xs), color=colorsX[\"Robust\"], linewidth=2.5)\n",
    "else:\n",
    "    ax.plot(xs, gaussian_kde(npv_robx)(xs), color=colorsX[\"BOTH\"], linewidth=2.5)\n",
    "    \n",
    "ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, pos: f\"{x/1000000:.0f}\"))\n",
    "ax.set_xlabel(\"Net Present Value (M NOK)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"NPV distribution across scenarios\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# --- (b) Win–loss by scenario (rotated) ---\n",
    "ax = axes[1]\n",
    "\n",
    "y = np.arange(len(diff_sorted))\n",
    "ax.scatter(diff_sorted, y, c=colors, label=\"Stochastic - Deterministic\")\n",
    "\n",
    "if TYPE != \"MAX_NPV\":\n",
    "    ax.scatter(diff_sorted_rob, y, c=colors_rob, \n",
    "               label=\"Robust - Deterministic\")\n",
    "ax.axvline(0, linewidth=1)\n",
    "\n",
    "ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, pos: f\"{x/1000000:.0f}\"))\n",
    "ax.set_ylabel(\"Scenario (sorted)\")\n",
    "ax.set_xlabel(\"NPV difference (M NOK)\")\n",
    "ax.set_title(\"Scenario-wise win–loss\")\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "if TYPE != \"MAX_NPV\":\n",
    "    ax.legend(frameon=False, loc=\"upper left\")\n",
    "\n",
    "# panel labels\n",
    "axes[0].text(    -0.12, 1.05, \"(a)\",    transform=axes[0].transAxes,    fontsize=12,    va=\"top\")\n",
    "axes[1].text(    -0.12, 1.05, \"(b)\",    transform=axes[1].transAxes,    fontsize=12,    va=\"top\")\n",
    "\n",
    "fig.savefig(    path_out+\"npv_ecdf_winloss_NPV\"+TYPE+\".png\",    bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "def merge_decision(df_reset, values_df, decision_col, label):\n",
    "    \"\"\"\n",
    "    Returns a merged dataframe with one decision column added.\n",
    "    \"\"\"\n",
    "\n",
    "    # decision dataframe\n",
    "    v = values_df.copy().reset_index()\n",
    "    v = v.rename(columns={\"level_0\": \"stand\", \"level_1\": \"schedule\"})\n",
    "    v = v[[\"stand\", \"schedule\", decision_col]]\n",
    "    v = v.rename(columns={decision_col: f\"{label}_value\"})\n",
    "\n",
    "    # base dataframe\n",
    "    base = df_reset.copy().reset_index()\n",
    "\n",
    "    merged = base.merge(\n",
    "        v,\n",
    "        on=[\"stand\", \"schedule\"],\n",
    "        how=\"outer\"\n",
    "    )\n",
    "\n",
    "    merged = merged.set_index([\"stand\", \"schedule\", \"sample\", \"period\"])\n",
    "\n",
    "    return merged\n",
    "\n",
    "merged_datasets = {}\n",
    "\n",
    "for label, values_df, decision_col in [\n",
    "    (\"T1\", T1_values_df, \"T1_value\"),\n",
    "    (\"T2\", T2_values_df, \"T2_value\"),\n",
    "    (\"T3\", T3_values_df, \"T3_value\"),\n",
    "]:\n",
    "    merged_datasets[label] = merge_decision(\n",
    "        df_reset,\n",
    "        values_df,\n",
    "        decision_col,\n",
    "        label\n",
    "    )\n",
    "\n",
    "# Access them\n",
    "merged_T1 = merged_datasets[\"T1\"]\n",
    "merged_T2 = merged_datasets[\"T2\"]\n",
    "merged_T3 = merged_datasets[\"T3\"]\n",
    "\n",
    "merged_all = df_reset.copy().reset_index()\n",
    "\n",
    "for label, values_df, decision_col in [\n",
    "    (\"T1\", T1_values_df, \"T1_value\"),\n",
    "    (\"T2\", T2_values_df, \"T2_value\"),\n",
    "    (\"T3\", T3_values_df, \"T3_value\"),\n",
    "]:\n",
    "    v = values_df.copy().reset_index()\n",
    "    v = v.rename(columns={\"level_0\": \"stand\", \"level_1\": \"schedule\"})\n",
    "    v = v[[\"stand\", \"schedule\", decision_col]]\n",
    "    v = v.rename(columns={decision_col: f\"{label}_value\"})\n",
    "\n",
    "    merged_all = merged_all.merge(v, on=[\"stand\", \"schedule\"], how=\"outer\")\n",
    "\n",
    "merged_all = merged_all.set_index([\"stand\", \"schedule\", \"sample\", \"period\"])\n",
    "\n",
    "tol = 1e-9  # adjust if needed\n",
    "\n",
    "m = merged_all.copy()\n",
    "\n",
    "# difference + boolean flag\n",
    "m[\"diff_T1_T2\"] = m[\"T1_value\"] - m[\"T2_value\"]\n",
    "m[\"is_diff_T1_T2\"] = m[\"diff_T1_T2\"].abs() > tol\n",
    "\n",
    "m[(m[\"is_diff_T1_T2\"]) & ((m[\"redID\"]>0) |(m[\"trID\"]>0) )].loc[(slice(None), slice(None), 0, slice(None)), :]\n",
    "\n",
    "### t1 -- stochastic\n",
    "#t2 -- deterministic\n",
    "#t3 -- Robust\n",
    "\n",
    "m[\"diff_T1_T2\"] = m[\"T1_value\"] - m[\"T2_value\"]\n",
    "m[\"diff_T3_T2\"] = m[\"T3_value\"] - m[\"T2_value\"]\n",
    "m[\"is_diff_T1_T2\"] = m[\"diff_T1_T2\"].abs() > tol\n",
    "m[\"is_diff_T3_T2\"] = m[\"diff_T3_T2\"].abs() > tol\n",
    "\n",
    "m[((m[\"redID\"]>0) |(m[\"trID\"]>0) )].loc[(slice(None), slice(None), 0, slice(None)), :]\n",
    "\n",
    "df = m.reset_index()\n",
    "df = df[df[\"sample\"] == 0]\n",
    "df = df[df[\"trID\"].isin(harvest_codes)]\n",
    "\n",
    "df[~((df[\"T1_value\"] == 0) &\n",
    "          (df[\"T2_value\"] == 0) &\n",
    "          (df[\"T3_value\"] == 0))]\n",
    "\n",
    "print(df[df['period']==1][(df['diff_T3_T2']>0)])\n",
    "print(df[df['period']==1][(df['diff_T3_T2']<0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "differe = \"T1_T2\"\n",
    "MAX_JUMP =3\n",
    "#differe = \"T3_T2\" # This will not work if running the max NPV case -- as T3 and T2 are the same.\n",
    "\n",
    "def add_flow_columns(\n",
    "    df1,\n",
    "    period_col=\"period\",\n",
    "    diff_col=\"diff_\"+differe,\n",
    "    group_col=\"stand\",\n",
    "    schedule_col=\"schedule\",\n",
    "    max_jump=MAX_JUMP,\n",
    "):\n",
    "    out = df1.copy()\n",
    "\n",
    "    # output columns (summary on one \"anchor\" row per stand)\n",
    "    out[\"time\"] = np.nan\n",
    "    out[\"difference\"] = np.nan\n",
    "    out[\"from to\"] = np.nan\n",
    "    out[\"from to\"] = out[\"from to\"].astype(\"object\")\n",
    "\n",
    "    # diagnostics (flags can be on multiple rows)\n",
    "    out[\"flag_jump_gt3\"] = False\n",
    "    out[\"jump_abs\"] = np.nan\n",
    "    out[\"flag_schedule_period_span_gt3\"] = False\n",
    "    out[\"schedule_period_span\"] = np.nan\n",
    "\n",
    "    # ---- 1) stand-level flow summary + jump flag ----\n",
    "    for stand, g in out.groupby(group_col, sort=False):\n",
    "        neg = g[g[diff_col] < 0]\n",
    "        pos = g[g[diff_col] > 0]\n",
    "\n",
    "        if not neg.empty:\n",
    "            idx_source = neg[diff_col].idxmin()\n",
    "            source_period = int(out.loc[idx_source, period_col])\n",
    "        else:\n",
    "            idx_source = None\n",
    "            source_period = None\n",
    "\n",
    "        if not pos.empty:\n",
    "            idx_sink = pos[diff_col].idxmax()\n",
    "            sink_period = int(out.loc[idx_sink, period_col])\n",
    "        else:\n",
    "            idx_sink = None\n",
    "            sink_period = None\n",
    "\n",
    "        if source_period is None and sink_period is None:\n",
    "            continue\n",
    "\n",
    "        # fallbacks (same as your current logic)\n",
    "        if source_period is None:\n",
    "            source_period = int(sink_period)\n",
    "            sink_period = max(source_period - 1, 1)\n",
    "\n",
    "        if sink_period is None:\n",
    "            sink_period = max(source_period - 1, 1)\n",
    "\n",
    "        jump_abs = abs(int(sink_period) - int(source_period))\n",
    "        flag_jump = jump_abs > max_jump\n",
    "\n",
    "        # anchor row choice (same as your current logic)\n",
    "        anchor_period = max(source_period, sink_period)\n",
    "        if anchor_period == sink_period and idx_sink is not None:\n",
    "            idx_anchor = idx_sink\n",
    "        elif idx_source is not None:\n",
    "            idx_anchor = idx_source\n",
    "        else:\n",
    "            idx_anchor = pos[diff_col].idxmax()\n",
    "\n",
    "        out.loc[idx_anchor, \"time\"] = int(source_period != sink_period)\n",
    "        out.loc[idx_anchor, \"difference\"] = out.loc[idx_anchor, diff_col]\n",
    "        out.loc[idx_anchor, \"from to\"] = f\"{source_period} to {sink_period}\"\n",
    "\n",
    "        out.loc[idx_anchor, \"jump_abs\"] = jump_abs\n",
    "        out.loc[idx_anchor, \"flag_jump_gt3\"] = flag_jump\n",
    "\n",
    "    # ---- 2) stand+schedule duplicate check: periods far apart ----\n",
    "    # mark ALL rows belonging to a (stand, schedule) group where span > max_jump AND group has >1 row\n",
    "    if schedule_col in out.columns:\n",
    "        sched_stats = (\n",
    "            out.groupby([group_col, schedule_col], sort=False)[period_col]\n",
    "               .agg(n=\"size\", pmin=\"min\", pmax=\"max\")\n",
    "               .reset_index()\n",
    "        )\n",
    "        sched_stats[\"period_span\"] = (sched_stats[\"pmax\"] - sched_stats[\"pmin\"]).astype(int)\n",
    "        sched_stats[\"flag_span_gt3\"] = (sched_stats[\"n\"] > 1) & (sched_stats[\"period_span\"] > max_jump)\n",
    "\n",
    "        flagged = sched_stats.loc[sched_stats[\"flag_span_gt3\"], [group_col, schedule_col, \"period_span\"]]\n",
    "\n",
    "        if not flagged.empty:\n",
    "            out = out.merge(flagged, on=[group_col, schedule_col], how=\"left\")\n",
    "            out[\"schedule_period_span\"] = out[\"period_span\"]\n",
    "            out[\"flag_schedule_period_span_gt3\"] = out[\"period_span\"].notna()\n",
    "            out.drop(columns=[\"period_span\"], inplace=True)\n",
    "        else:\n",
    "            out[\"schedule_period_span\"] = np.nan\n",
    "            out[\"flag_schedule_period_span_gt3\"] = False\n",
    "\n",
    "    return out\n",
    "\n",
    "df1 = (\n",
    "    df[df[\"diff_\"+differe] != 0]\n",
    "      .sort_values([\"stand\",  \"schedule\",\"period\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "#df1=df1[df1['period']>6]\n",
    "MAX_JUMP = 3  # anything greater than this should be checked\n",
    "\n",
    "df2 = add_flow_columns(df1, period_col=\"period\", diff_col=\"diff_\"+differe, group_col=\"stand\")\n",
    "\n",
    "flows = df2.dropna(subset=[\"from to\"]).copy()\n",
    "\n",
    "tmp = flows[\"from to\"].str.extract(r\"^\\s*(\\d+)\\s+to\\s+(\\d+)\\s*$\")\n",
    "flows[\"from_period\"] = tmp[0].astype(int)\n",
    "flows[\"to_period\"] = tmp[1].astype(int)\n",
    "\n",
    "# ignore same-period flows\n",
    "flows = flows[flows[\"from_period\"] != flows[\"to_period\"]]\n",
    "\n",
    "# use absolute value\n",
    "flows[\"abs_difference\"] = flows[\"difference\"].abs()\n",
    "\n",
    "flow_sum = (\n",
    "    flows.groupby([\"from_period\", \"to_period\"], as_index=False)[\"abs_difference\"]\n",
    "         .sum()\n",
    ")\n",
    "\n",
    "# keep NaN for non-existing transitions\n",
    "M = (\n",
    "    flow_sum.pivot(index=\"from_period\", columns=\"to_period\", values=\"abs_difference\")\n",
    "            .reindex(index=range(1, 13), columns=range(1, 13))\n",
    ")\n",
    "\n",
    "df1 = (\n",
    "    df[df[\"diff_\"+differe] != 0]\n",
    "      .sort_values([\"stand\",  \"schedule\",\"period\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "df1=df1[df1['period']>6]\n",
    "MAX_JUMP = 3  # anything greater than this should be checked\n",
    "\n",
    "df2 = add_flow_columns(df1, period_col=\"period\", diff_col=\"diff_\"+differe, group_col=\"stand\")\n",
    "\n",
    "flows = df2.dropna(subset=[\"from to\"]).copy()\n",
    "\n",
    "tmp = flows[\"from to\"].str.extract(r\"^\\s*(\\d+)\\s+to\\s+(\\d+)\\s*$\")\n",
    "flows[\"from_period\"] = tmp[0].astype(int)\n",
    "flows[\"to_period\"] = tmp[1].astype(int)\n",
    "\n",
    "# ignore same-period flows\n",
    "flows = flows[flows[\"from_period\"] != flows[\"to_period\"]]\n",
    "\n",
    "# use absolute value\n",
    "flows[\"abs_difference\"] = flows[\"difference\"].abs()\n",
    "\n",
    "flow_sum = (\n",
    "    flows.groupby([\"from_period\", \"to_period\"], as_index=False)[\"abs_difference\"]\n",
    "         .sum()\n",
    ")\n",
    "\n",
    "# keep NaN for non-existing transitions\n",
    "Ma = (\n",
    "    flow_sum.pivot(index=\"from_period\", columns=\"to_period\", values=\"abs_difference\")\n",
    "            .reindex(index=range(1, 13), columns=range(1, 13))\n",
    ")\n",
    "\n",
    "rows = [11, 12]\n",
    "cols = [11, 12]\n",
    "\n",
    "M.loc[rows, cols] = Ma.loc[rows, cols]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 1,\n",
    "    figsize=(6, 6),   # width roughly 2x height\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "vmax = np.nanmax([np.nanmax(M.values*902),\n",
    "                  np.nanmax(M.values*902)])\n",
    "\n",
    "# ---------- First matrix ----------\n",
    "data = M.values * 902\n",
    "im0 = axes.imshow(data, aspect=\"equal\", vmin=0, vmax=vmax)\n",
    "\n",
    "axes.set_xticks(range(12))\n",
    "axes.set_xticklabels(range(1, 13))\n",
    "axes.set_yticks(range(12))\n",
    "axes.set_yticklabels(range(1, 13))\n",
    "axes.set_xlabel(\"To period\")\n",
    "axes.set_ylabel(\"From period\")\n",
    "\n",
    "axes.set_xlim(-0.5, 11.5)\n",
    "axes.set_ylim(11.5, -0.5)\n",
    "\n",
    "norm = im0.norm\n",
    "cmap = im0.cmap\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        value = data[i, j]\n",
    "        if not np.isnan(value) and value > 0:\n",
    "            rgba = cmap(norm(value))\n",
    "            brightness = 0.299*rgba[0] + 0.587*rgba[1] + 0.114*rgba[2]\n",
    "            text_color = \"black\" if brightness > 0.5 else \"white\"\n",
    "\n",
    "            axes.text(\n",
    "                j, i,\n",
    "                f\"{value:,.0f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=text_color,\n",
    "                fontsize=8,\n",
    "                fontweight=\"bold\"\n",
    "            )\n",
    "            n = data.shape[0]  # assuming square matrix            \n",
    "            axes.plot(\n",
    "                [-0.5, n - 0.5],\n",
    "                [-0.5, n - 0.5],\n",
    "                color=\"black\",\n",
    "                linewidth=2,\n",
    "                linestyle=\"--\"   # optional\n",
    "            )   \n",
    "\n",
    "# ---------- Shared colorbar ----------\n",
    "cbar = fig.colorbar(\n",
    "    im0,\n",
    "    ax=axes,\n",
    "    fraction=0.045,   # keeps colorbar proportional\n",
    "    pad=0.04,\n",
    "    label=\"Area with a change of harvesting (ha)\"\n",
    ")\n",
    "\n",
    "\n",
    "plt.savefig(\n",
    "    path_out + \"BALANCE\" + TYPE + \"_\" + differe + \".png\",\n",
    "    dpi=300\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct even flow analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Maximize for 3000 m3\n",
    "#Minmimize deviations -- 3 approaches\n",
    "#Stochastic:\n",
    "TYPE = \"EVEN_FLOW\"\n",
    "t1.model1.target_down = 600000*5#3000#50000/900\n",
    "def outcome_rule(model1):\n",
    "    NPV = t1.model1.NPV-t1.model1.CVAR_down_Tot *1000\n",
    "    return NPV\n",
    "t1.model1.OBJ = Objective(rule=outcome_rule, sense=maximize)\n",
    "t1.solve()\n",
    "\n",
    "#Deterministic\n",
    "t2.model1.DEC_inc_limit = 600000*5#3000#50000/900\n",
    "t2.solve()\n",
    "\n",
    "\n",
    "#Robust\n",
    "t3.model1.DEC_inc_robust = 600000*5#3000#50000/900\n",
    "\n",
    "for p in t3.model1.year:\n",
    "    t3.model1.Gamma_t[p] = 3.5# Need to consider how to select the gamma parameter. Could reflect on Palma and nelson.\n",
    "t3.solve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "#Calculation of core values:\n",
    "#Resampling to ensure anticipatory planning doesn't occur.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optimized function to generate random values for sample\n",
    "def sample_random(size):\n",
    "    return np.random.choice(range(100), size=size, replace=False)\n",
    "\n",
    "# Optimized version of create_sample\n",
    "def create_sample(df):\n",
    "    df_reset = df.reset_index()\n",
    "    unique_stands = df_reset['stand'].unique()\n",
    "    \n",
    "    # Loop through each stand and replace the sample values\n",
    "    for stand in unique_stands:\n",
    "        # Create a mask for the current stand\n",
    "        mask = df_reset['stand'] == stand\n",
    "        \n",
    "        # Get the unique sample values for this stand\n",
    "        unique_samples = df_reset.loc[mask, 'sample'].unique()\n",
    "        \n",
    "        # Generate random values for the current stand's sample values\n",
    "        #random_mapping = dict(zip(unique_samples, sample_random(100)))#len(unique_samples))))\n",
    "        \n",
    "        df_x = df_reset[df_reset['stand'] ==stand]#, 'sample'] = df_reset4.loc[mask, 'sample'].map(random_mapping)\n",
    "        sample=sample_random(100)\n",
    "        df_x =df_x[df_x['sample'].isin(sample)]\n",
    "        order = {s: i for i, s in enumerate(sample)}\n",
    "\n",
    "        # sort df_x according to sample order\n",
    "        df_x = df_x.sort_values(\n",
    "            by='sample',\n",
    "            key=lambda s: s.map(order)\n",
    "        )\n",
    "        \n",
    "        df_x['sample'] = pd.factorize(df_x['sample'])[0]\n",
    "        \n",
    "        if stand == unique_stands[0]:\n",
    "            df_y = df_x\n",
    "        else:\n",
    "            df_y = pd.concat([df_y,df_x])\n",
    "    return df_y\n",
    "\n",
    "# Constructing a sample:\n",
    "df_reset = create_sample(df=t1.data_orig)\n",
    "\n",
    "df_reset = df_reset[df_reset['sample']!=-1]\n",
    "df_reset = df_reset.set_index(['stand', 'schedule','sample', 'period'])\n",
    "\n",
    "# Pre-fetching all necessary data to avoid repeated lookups\n",
    "npv_data = df_reset.npv\n",
    "npv_data_det = t2.all_data.npv\n",
    "\n",
    "VOL_t_data = df_reset.vol_t\n",
    "VOL_t_data_det = t2.all_data.vol_t\n",
    "\n",
    "# Convert X1_values to a Pandas DataFrame\n",
    "#T1 -- is the stochastic solution\n",
    "T1_values_dict = {index: t1.model1.X1[index].value for index in t1.model1.X1}\n",
    "T1_values_df = pd.DataFrame.from_dict(T1_values_dict, orient='index', columns=['T1_value'])\n",
    "T1_values_df.index = pd.MultiIndex.from_tuples(T1_values_df.index)  # Convert index to MultiIndex\n",
    "\n",
    "#T2 -- is the deterministic solution\n",
    "T2_values_dict = {index: t2.model1.X1[index].value for index in t2.model1.X1}\n",
    "T2_values_df = pd.DataFrame.from_dict(T2_values_dict, orient='index', columns=['T2_value'])\n",
    "T2_values_df.index = pd.MultiIndex.from_tuples(T2_values_df.index)  # Convert index to MultiIndex if (s, r) is a tuple\n",
    "\n",
    "#T3 -- is the robust solution\n",
    "T3_values_dict = {index: t3.model1.X1[index].value for index in t3.model1.X1}\n",
    "T3_values_df = pd.DataFrame.from_dict(T3_values_dict, orient='index', columns=['T3_value'])\n",
    "T3_values_df.index = pd.MultiIndex.from_tuples(T3_values_df.index)  # Convert index to MultiIndex if (s, r) is a tuple\n",
    "\n",
    "T1_values_df = T1_values_df.copy()\n",
    "T2_values_df = T2_values_df.copy()\n",
    "T3_values_df = T3_values_df.copy()\n",
    "\n",
    "T1_values_df.index = T1_values_df.index.set_names(['stand', 'schedule'])\n",
    "T2_values_df.index = T2_values_df.index.set_names(['stand', 'schedule'])\n",
    "T3_values_df.index = T3_values_df.index.set_names(['stand', 'schedule'])\n",
    "\n",
    "npv_sto,npv_det, npv_rob = [],[],[]\n",
    "ef_sto, ef_det, ef_rob = {},{},{}\n",
    "\n",
    "for i in range(0, 100):\n",
    "    npv_sto =npv_sto+[(float(T1_values_df.mul(npv_data.loc[slice(None), slice(None), i, 1], axis=0).sum(axis=0).item()))*901.238]\n",
    "    npv_det =npv_det+[(float(T2_values_df.mul(npv_data.loc[slice(None), slice(None), i, 1], axis=0).sum(axis=0).item()))*901.238]\n",
    "    npv_rob =npv_rob+[(float(T3_values_df.mul(npv_data.loc[slice(None), slice(None), i, 1], axis=0).sum(axis=0).item()))*901.238]\n",
    "\n",
    "for k in range(1,13):\n",
    "    ef_sto[k] = [float(T1_values_df.mul(VOL_t_data.loc[slice(None), slice(None), i, k], axis=0).sum(axis=0).item())*901.238   for i in range(0, 100)]\n",
    "    ef_det[k] = [float(T2_values_df.mul(VOL_t_data.loc[slice(None), slice(None), i, k], axis=0).sum(axis=0).item())*901.238   for i in range(0, 100)]\n",
    "    ef_rob[k] = [float(T3_values_df.mul(VOL_t_data.loc[slice(None), slice(None), i, k], axis=0).sum(axis=0).item())*901.238   for i in range(0, 100)]\n",
    "    \n",
    "print(\"Stochastic\",(float(T1_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item())))\n",
    "print(\"Deterministic\", (float(T2_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item())))\n",
    "print(\"Robust\", (float(T3_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item())))\n",
    "\n",
    "print(\"Perceived decline in stochastic solution:\", (float(T1_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item()))/(float(T2_values_df.mul(npv_data_det.loc[slice(None), slice(None), 0, 1], axis=0).sum(axis=0).item())))\n",
    "\n",
    "print(\"Stochastic\",(np.mean([npv_sto])))\n",
    "print(\"Deterministic\", (np.mean([npv_det])))\n",
    "print(\"Robust\", (np.mean([npv_rob])))\n",
    "print(\"Stochastic solution improvement:\", (np.mean([npv_sto]))/(np.mean([npv_det])))\n",
    "print(\"Stochastic solution improvement:\", (np.mean([npv_sto]))/(np.mean([npv_rob])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "##NPV ANALYSIS -- constructing the figures that present the distribution of the NPV and the scenario wise comparison\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.stats import gaussian_kde\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sto_color = \"#0072B2\"  # blue\n",
    "rob_color = \"#D55E00\"  # vermillion\n",
    "\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, len(x) + 1) / len(x)\n",
    "    return x, y\n",
    "\n",
    "# scenario-wise NPVs\n",
    "npv_detx = np.array(npv_det)\n",
    "npv_stox = np.array(npv_sto)\n",
    "npv_robx = np.array(npv_rob)\n",
    "\n",
    "diff_sorted = np.sort(npv_stox - npv_detx)\n",
    "diff_sorted_rob = np.sort(npv_stox - npv_robx)\n",
    "\n",
    "colors = ['tab:blue' if d > 0 else 'tab:red' for d in diff_sorted]\n",
    "\n",
    "\n",
    "def lighten(color, amount=0.5):\n",
    "    c = mcolors.to_rgb(color)\n",
    "    return tuple(1 - amount*(1 - x) for x in c)\n",
    "\n",
    "light_blue = lighten('tab:blue', 0.4)\n",
    "light_red  = lighten('tab:red', 0.4)\n",
    "\n",
    "colors_rob = [light_blue if d > 0 else light_red for d in diff_sorted_rob]\n",
    "\n",
    "# scenario-wise NPVs\n",
    "npv_detx = np.array(npv_det)\n",
    "npv_stox = np.array(npv_sto)\n",
    "npv_robx = np.array(npv_rob)\n",
    "\n",
    "# ---- figure with two panels ----\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), constrained_layout=True)\n",
    "\n",
    "# --- (a) Distribution (hist + optional KDE) ---\n",
    "ax = axes[0]\n",
    "# Define one base color per scenario\n",
    "colorsX = {    \"Stochastic\": \"#1f77b4\",   \"Deterministic\": \"#d62728\",    \"Robust\": \"#2ca02c\",\"BOTH\": \"red\"}\n",
    "\n",
    "all_vals = np.concatenate([npv_stox, npv_robx, npv_detx])\n",
    "bins = np.histogram_bin_edges(all_vals, bins=\"fd\")\n",
    "bins = 50\n",
    "# Histograms (light + transparent)\n",
    "ax.hist(npv_stox, bins=bins, density=True,  color=colorsX[\"Stochastic\"], alpha=0.30, label=\"Stochastic\")\n",
    "if TYPE != \"MAX_NPV\":\n",
    "    ax.hist(npv_detx, bins=bins, density=True, color=colorsX[\"Deterministic\"], alpha=0.30, label=\"Deterministic\")\n",
    "    ax.hist(npv_robx, bins=bins, density=True, color=colorsX[\"Robust\"], alpha=0.30, label=\"Robust\")\n",
    "else:\n",
    "    ax.hist(npv_robx, bins=bins, density=True, color=colorsX[\"BOTH\"], alpha=0.30, label=\"Deterministic and Robust\")\n",
    "# KDE lines (same color, darker + thicker)\n",
    "xs = np.linspace(all_vals.min(), all_vals.max(), 300)\n",
    "\n",
    "ax.plot(xs, gaussian_kde(npv_stox)(xs), color=colorsX[\"Stochastic\"], linewidth=2.5)\n",
    "if TYPE != \"MAX_NPV\":\n",
    "    ax.plot(xs, gaussian_kde(npv_detx)(xs), color=colorsX[\"Deterministic\"], linewidth=2.5)\n",
    "    ax.plot(xs, gaussian_kde(npv_robx)(xs), color=colorsX[\"Robust\"], linewidth=2.5)\n",
    "else:\n",
    "    ax.plot(xs, gaussian_kde(npv_robx)(xs), color=colorsX[\"BOTH\"], linewidth=2.5)\n",
    "    \n",
    "ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, pos: f\"{x/1000000:.0f}\"))\n",
    "ax.set_xlabel(\"Net Present Value (M NOK)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"NPV distribution across scenarios\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# --- (b) Win–loss by scenario (rotated) ---\n",
    "ax = axes[1]\n",
    "\n",
    "y = np.arange(len(diff_sorted))\n",
    "ax.scatter(diff_sorted, y, c=colors, label=\"Stochastic - Deterministic\")\n",
    "\n",
    "if TYPE != \"MAX_NPV\":\n",
    "    ax.scatter(diff_sorted_rob, y, c=colors_rob, \n",
    "               label=\"Robust - Deterministic\")\n",
    "ax.axvline(0, linewidth=1)\n",
    "\n",
    "ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, pos: f\"{x/1000000:.0f}\"))\n",
    "ax.set_ylabel(\"Scenario (sorted)\")\n",
    "ax.set_xlabel(\"NPV difference (M NOK)\")\n",
    "ax.set_title(\"Scenario-wise win–loss\")\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "if TYPE != \"MAX_NPV\":\n",
    "    ax.legend(frameon=False, loc=\"upper left\")\n",
    "\n",
    "# panel labels\n",
    "axes[0].text(    -0.12, 1.05, \"(a)\",    transform=axes[0].transAxes,    fontsize=12,    va=\"top\")\n",
    "axes[1].text(    -0.12, 1.05, \"(b)\",    transform=axes[1].transAxes,    fontsize=12,    va=\"top\")\n",
    "\n",
    "fig.savefig(    path_out+\"npv_ecdf_winloss_NPV\"+TYPE+\".png\",    bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def merge_decision(df_reset, values_df, decision_col, label):\n",
    "    \"\"\"\n",
    "    Returns a merged dataframe with one decision column added.\n",
    "    \"\"\"\n",
    "\n",
    "    # decision dataframe\n",
    "    v = values_df.copy().reset_index()\n",
    "    v = v.rename(columns={\"level_0\": \"stand\", \"level_1\": \"schedule\"})\n",
    "    v = v[[\"stand\", \"schedule\", decision_col]]\n",
    "    v = v.rename(columns={decision_col: f\"{label}_value\"})\n",
    "\n",
    "    # base dataframe\n",
    "    base = df_reset.copy().reset_index()\n",
    "\n",
    "    merged = base.merge(\n",
    "        v,\n",
    "        on=[\"stand\", \"schedule\"],\n",
    "        how=\"outer\"\n",
    "    )\n",
    "\n",
    "    merged = merged.set_index([\"stand\", \"schedule\", \"sample\", \"period\"])\n",
    "\n",
    "    return merged\n",
    "\n",
    "merged_datasets = {}\n",
    "\n",
    "for label, values_df, decision_col in [\n",
    "    (\"T1\", T1_values_df, \"T1_value\"),\n",
    "    (\"T2\", T2_values_df, \"T2_value\"),\n",
    "    (\"T3\", T3_values_df, \"T3_value\"),\n",
    "]:\n",
    "    merged_datasets[label] = merge_decision(\n",
    "        df_reset,\n",
    "        values_df,\n",
    "        decision_col,\n",
    "        label\n",
    "    )\n",
    "\n",
    "# Access them\n",
    "merged_T1 = merged_datasets[\"T1\"]\n",
    "merged_T2 = merged_datasets[\"T2\"]\n",
    "merged_T3 = merged_datasets[\"T3\"]\n",
    "\n",
    "merged_all = df_reset.copy().reset_index()\n",
    "\n",
    "for label, values_df, decision_col in [\n",
    "    (\"T1\", T1_values_df, \"T1_value\"),\n",
    "    (\"T2\", T2_values_df, \"T2_value\"),\n",
    "    (\"T3\", T3_values_df, \"T3_value\"),\n",
    "]:\n",
    "    v = values_df.copy().reset_index()\n",
    "    v = v.rename(columns={\"level_0\": \"stand\", \"level_1\": \"schedule\"})\n",
    "    v = v[[\"stand\", \"schedule\", decision_col]]\n",
    "    v = v.rename(columns={decision_col: f\"{label}_value\"})\n",
    "\n",
    "    merged_all = merged_all.merge(v, on=[\"stand\", \"schedule\"], how=\"outer\")\n",
    "\n",
    "merged_all = merged_all.set_index([\"stand\", \"schedule\", \"sample\", \"period\"])\n",
    "\n",
    "tol = 1e-9  # adjust if needed\n",
    "\n",
    "m = merged_all.copy()\n",
    "\n",
    "# difference + boolean flag\n",
    "m[\"diff_T1_T2\"] = m[\"T1_value\"] - m[\"T2_value\"]\n",
    "m[\"is_diff_T1_T2\"] = m[\"diff_T1_T2\"].abs() > tol\n",
    "\n",
    "m[(m[\"is_diff_T1_T2\"]) & ((m[\"redID\"]>0) |(m[\"trID\"]>0) )].loc[(slice(None), slice(None), 0, slice(None)), :]\n",
    "\n",
    "### t1 -- stochastic\n",
    "#t2 -- deterministic\n",
    "#t3 -- Robust\n",
    "\n",
    "m[\"diff_T1_T2\"] = m[\"T1_value\"] - m[\"T2_value\"]\n",
    "m[\"diff_T3_T2\"] = m[\"T3_value\"] - m[\"T2_value\"]\n",
    "m[\"is_diff_T1_T2\"] = m[\"diff_T1_T2\"].abs() > tol\n",
    "m[\"is_diff_T3_T2\"] = m[\"diff_T3_T2\"].abs() > tol\n",
    "\n",
    "m[((m[\"redID\"]>0) |(m[\"trID\"]>0) )].loc[(slice(None), slice(None), 0, slice(None)), :]\n",
    "\n",
    "df = m.reset_index()\n",
    "df = df[df[\"sample\"] == 0]\n",
    "df = df[df[\"trID\"].isin(harvest_codes)]\n",
    "\n",
    "df[~((df[\"T1_value\"] == 0) &\n",
    "          (df[\"T2_value\"] == 0) &\n",
    "          (df[\"T3_value\"] == 0))]\n",
    "\n",
    "print(df[df['period']==1][(df['diff_T3_T2']>0)])\n",
    "print(df[df['period']==1][(df['diff_T3_T2']<0)])\n",
    "\n",
    "# Used to construct the temporal figures, difference from deterministic case. A minor issue remains -- \n",
    "# if multiple harvest times, the later harvest times may be removed. A simple approach to extract the values\n",
    "# is to run without the first few periods, and compare the plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "differe = \"T1_T2\"\n",
    "#differe = \"T3_T2\" # This will not work if running the max NPV case -- as T3 and T2 are the same.\n",
    "\n",
    "def add_flow_columns(\n",
    "    df1,\n",
    "    period_col=\"period\",\n",
    "    diff_col=\"diff_\"+differe,\n",
    "    group_col=\"stand\",\n",
    "    schedule_col=\"schedule\",\n",
    "    max_jump=MAX_JUMP,\n",
    "):\n",
    "    out = df1.copy()\n",
    "\n",
    "    # output columns (summary on one \"anchor\" row per stand)\n",
    "    out[\"time\"] = np.nan\n",
    "    out[\"difference\"] = np.nan\n",
    "    out[\"from to\"] = np.nan\n",
    "    out[\"from to\"] = out[\"from to\"].astype(\"object\")\n",
    "\n",
    "    # diagnostics (flags can be on multiple rows)\n",
    "    out[\"flag_jump_gt3\"] = False\n",
    "    out[\"jump_abs\"] = np.nan\n",
    "    out[\"flag_schedule_period_span_gt3\"] = False\n",
    "    out[\"schedule_period_span\"] = np.nan\n",
    "\n",
    "    # ---- 1) stand-level flow summary + jump flag ----\n",
    "    for stand, g in out.groupby(group_col, sort=False):\n",
    "        neg = g[g[diff_col] < 0]\n",
    "        pos = g[g[diff_col] > 0]\n",
    "\n",
    "        if not neg.empty:\n",
    "            idx_source = neg[diff_col].idxmin()\n",
    "            source_period = int(out.loc[idx_source, period_col])\n",
    "        else:\n",
    "            idx_source = None\n",
    "            source_period = None\n",
    "\n",
    "        if not pos.empty:\n",
    "            idx_sink = pos[diff_col].idxmax()\n",
    "            sink_period = int(out.loc[idx_sink, period_col])\n",
    "        else:\n",
    "            idx_sink = None\n",
    "            sink_period = None\n",
    "\n",
    "        if source_period is None and sink_period is None:\n",
    "            continue\n",
    "\n",
    "        # fallbacks (same as your current logic)\n",
    "        if source_period is None:\n",
    "            source_period = int(sink_period)\n",
    "            sink_period = max(source_period - 1, 1)\n",
    "\n",
    "        if sink_period is None:\n",
    "            sink_period = max(source_period - 1, 1)\n",
    "\n",
    "        jump_abs = abs(int(sink_period) - int(source_period))\n",
    "        flag_jump = jump_abs > max_jump\n",
    "\n",
    "        # anchor row choice (same as your current logic)\n",
    "        anchor_period = max(source_period, sink_period)\n",
    "        if anchor_period == sink_period and idx_sink is not None:\n",
    "            idx_anchor = idx_sink\n",
    "        elif idx_source is not None:\n",
    "            idx_anchor = idx_source\n",
    "        else:\n",
    "            idx_anchor = pos[diff_col].idxmax()\n",
    "\n",
    "        out.loc[idx_anchor, \"time\"] = int(source_period != sink_period)\n",
    "        out.loc[idx_anchor, \"difference\"] = out.loc[idx_anchor, diff_col]\n",
    "        out.loc[idx_anchor, \"from to\"] = f\"{source_period} to {sink_period}\"\n",
    "\n",
    "        out.loc[idx_anchor, \"jump_abs\"] = jump_abs\n",
    "        out.loc[idx_anchor, \"flag_jump_gt3\"] = flag_jump\n",
    "\n",
    "    # ---- 2) stand+schedule duplicate check: periods far apart ----\n",
    "    # mark ALL rows belonging to a (stand, schedule) group where span > max_jump AND group has >1 row\n",
    "    if schedule_col in out.columns:\n",
    "        sched_stats = (\n",
    "            out.groupby([group_col, schedule_col], sort=False)[period_col]\n",
    "               .agg(n=\"size\", pmin=\"min\", pmax=\"max\")\n",
    "               .reset_index()\n",
    "        )\n",
    "        sched_stats[\"period_span\"] = (sched_stats[\"pmax\"] - sched_stats[\"pmin\"]).astype(int)\n",
    "        sched_stats[\"flag_span_gt3\"] = (sched_stats[\"n\"] > 1) & (sched_stats[\"period_span\"] > max_jump)\n",
    "\n",
    "        flagged = sched_stats.loc[sched_stats[\"flag_span_gt3\"], [group_col, schedule_col, \"period_span\"]]\n",
    "\n",
    "        if not flagged.empty:\n",
    "            out = out.merge(flagged, on=[group_col, schedule_col], how=\"left\")\n",
    "            out[\"schedule_period_span\"] = out[\"period_span\"]\n",
    "            out[\"flag_schedule_period_span_gt3\"] = out[\"period_span\"].notna()\n",
    "            out.drop(columns=[\"period_span\"], inplace=True)\n",
    "        else:\n",
    "            out[\"schedule_period_span\"] = np.nan\n",
    "            out[\"flag_schedule_period_span_gt3\"] = False\n",
    "\n",
    "    return out\n",
    "\n",
    "df1 = (\n",
    "    df[df[\"diff_\"+differe] != 0]\n",
    "      .sort_values([\"stand\",  \"schedule\",\"period\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "#df1=df1[df1['period']>6]\n",
    "MAX_JUMP = 3  # anything greater than this should be checked\n",
    "\n",
    "df2 = add_flow_columns(df1, period_col=\"period\", diff_col=\"diff_\"+differe, group_col=\"stand\")\n",
    "\n",
    "flows = df2.dropna(subset=[\"from to\"]).copy()\n",
    "\n",
    "tmp = flows[\"from to\"].str.extract(r\"^\\s*(\\d+)\\s+to\\s+(\\d+)\\s*$\")\n",
    "flows[\"from_period\"] = tmp[0].astype(int)\n",
    "flows[\"to_period\"] = tmp[1].astype(int)\n",
    "\n",
    "# ignore same-period flows\n",
    "flows = flows[flows[\"from_period\"] != flows[\"to_period\"]]\n",
    "\n",
    "# use absolute value\n",
    "flows[\"abs_difference\"] = flows[\"difference\"].abs()\n",
    "\n",
    "flow_sum = (\n",
    "    flows.groupby([\"from_period\", \"to_period\"], as_index=False)[\"abs_difference\"]\n",
    "         .sum()\n",
    ")\n",
    "\n",
    "# keep NaN for non-existing transitions\n",
    "M = (\n",
    "    flow_sum.pivot(index=\"from_period\", columns=\"to_period\", values=\"abs_difference\")\n",
    "            .reindex(index=range(1, 13), columns=range(1, 13))\n",
    ")\n",
    "\n",
    "df1 = (\n",
    "    df[df[\"diff_\"+differe] != 0]\n",
    "      .sort_values([\"stand\",  \"schedule\",\"period\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "df1=df1[df1['period']>6]\n",
    "MAX_JUMP = 3  # anything greater than this should be checked\n",
    "\n",
    "df2 = add_flow_columns(df1, period_col=\"period\", diff_col=\"diff_\"+differe, group_col=\"stand\")\n",
    "\n",
    "flows = df2.dropna(subset=[\"from to\"]).copy()\n",
    "\n",
    "tmp = flows[\"from to\"].str.extract(r\"^\\s*(\\d+)\\s+to\\s+(\\d+)\\s*$\")\n",
    "flows[\"from_period\"] = tmp[0].astype(int)\n",
    "flows[\"to_period\"] = tmp[1].astype(int)\n",
    "\n",
    "# ignore same-period flows\n",
    "flows = flows[flows[\"from_period\"] != flows[\"to_period\"]]\n",
    "\n",
    "# use absolute value\n",
    "flows[\"abs_difference\"] = flows[\"difference\"].abs()\n",
    "\n",
    "flow_sum = (\n",
    "    flows.groupby([\"from_period\", \"to_period\"], as_index=False)[\"abs_difference\"]\n",
    "         .sum()\n",
    ")\n",
    "\n",
    "# keep NaN for non-existing transitions\n",
    "Ma = (\n",
    "    flow_sum.pivot(index=\"from_period\", columns=\"to_period\", values=\"abs_difference\")\n",
    "            .reindex(index=range(1, 13), columns=range(1, 13))\n",
    ")\n",
    "\n",
    "rows = [11, 12]\n",
    "cols = [11, 12]\n",
    "\n",
    "M.loc[rows, cols] = Ma.loc[rows, cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "differe = \"T3_T2\"\n",
    "\n",
    "df1 = (\n",
    "    df[df[\"diff_\"+differe] != 0]\n",
    "      .sort_values([\"stand\",  \"schedule\",\"period\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "#df1=df1[df1['period']>6]\n",
    "MAX_JUMP = 3  # anything greater than this should be checked\n",
    "\n",
    "df2 = add_flow_columns(df1, period_col=\"period\", diff_col=\"diff_\"+differe, group_col=\"stand\")\n",
    "\n",
    "flows = df2.dropna(subset=[\"from to\"]).copy()\n",
    "\n",
    "tmp = flows[\"from to\"].str.extract(r\"^\\s*(\\d+)\\s+to\\s+(\\d+)\\s*$\")\n",
    "flows[\"from_period\"] = tmp[0].astype(int)\n",
    "flows[\"to_period\"] = tmp[1].astype(int)\n",
    "\n",
    "# ignore same-period flows\n",
    "flows = flows[flows[\"from_period\"] != flows[\"to_period\"]]\n",
    "\n",
    "# use absolute value\n",
    "flows[\"abs_difference\"] = flows[\"difference\"].abs()\n",
    "\n",
    "flow_sum = (\n",
    "    flows.groupby([\"from_period\", \"to_period\"], as_index=False)[\"abs_difference\"]\n",
    "         .sum()\n",
    ")\n",
    "\n",
    "# keep NaN for non-existing transitions\n",
    "M1 = (\n",
    "    flow_sum.pivot(index=\"from_period\", columns=\"to_period\", values=\"abs_difference\")\n",
    "            .reindex(index=range(1, 13), columns=range(1, 13))\n",
    ")\n",
    "\n",
    "df1 = (\n",
    "    df[df[\"diff_\"+differe] != 0]\n",
    "      .sort_values([\"stand\",  \"schedule\",\"period\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "df1=df1[df1['period']>6]\n",
    "MAX_JUMP = 3  # anything greater than this should be checked\n",
    "\n",
    "df2 = add_flow_columns(df1, period_col=\"period\", diff_col=\"diff_\"+differe, group_col=\"stand\")\n",
    "\n",
    "flows = df2.dropna(subset=[\"from to\"]).copy()\n",
    "\n",
    "tmp = flows[\"from to\"].str.extract(r\"^\\s*(\\d+)\\s+to\\s+(\\d+)\\s*$\")\n",
    "flows[\"from_period\"] = tmp[0].astype(int)\n",
    "flows[\"to_period\"] = tmp[1].astype(int)\n",
    "\n",
    "# ignore same-period flows\n",
    "flows = flows[flows[\"from_period\"] != flows[\"to_period\"]]\n",
    "\n",
    "# use absolute value\n",
    "flows[\"abs_difference\"] = flows[\"difference\"].abs()\n",
    "\n",
    "flow_sum = (\n",
    "    flows.groupby([\"from_period\", \"to_period\"], as_index=False)[\"abs_difference\"]\n",
    "         .sum()\n",
    ")\n",
    "\n",
    "# keep NaN for non-existing transitions\n",
    "M1a = (\n",
    "    flow_sum.pivot(index=\"from_period\", columns=\"to_period\", values=\"abs_difference\")\n",
    "            .reindex(index=range(1, 13), columns=range(1, 13))\n",
    ")\n",
    "\n",
    "rows = [11, 12]\n",
    "cols = [11, 12]\n",
    "\n",
    "M1.loc[rows, cols] = M1a.loc[rows, cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, 2,\n",
    "    figsize=(12, 6),   # width roughly 2x height\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "vmax = np.nanmax([np.nanmax(M.values*902),\n",
    "                  np.nanmax(M1.values*902)])\n",
    "\n",
    "# ---------- First matrix ----------\n",
    "data = M.values * 902\n",
    "im0 = axes[0].imshow(data, aspect=\"equal\", vmin=0, vmax=vmax)\n",
    "\n",
    "axes[0].set_xticks(range(12))\n",
    "axes[0].set_xticklabels(range(1, 13))\n",
    "axes[0].set_yticks(range(12))\n",
    "axes[0].set_yticklabels(range(1, 13))\n",
    "axes[0].set_xlabel(\"To period\")\n",
    "axes[0].set_ylabel(\"From period\")\n",
    "axes[0].set_title(\"Stochastic v Deterministic\")\n",
    "\n",
    "axes[0].set_xlim(-0.5, 11.5)\n",
    "axes[0].set_ylim(11.5, -0.5)\n",
    "\n",
    "norm = im0.norm\n",
    "cmap = im0.cmap\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        value = data[i, j]\n",
    "        if not np.isnan(value) and value > 0:\n",
    "            rgba = cmap(norm(value))\n",
    "            brightness = 0.299*rgba[0] + 0.587*rgba[1] + 0.114*rgba[2]\n",
    "            text_color = \"black\" if brightness > 0.5 else \"white\"\n",
    "\n",
    "            axes[0].text(\n",
    "                j, i,\n",
    "                f\"{value:,.0f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=text_color,\n",
    "                fontsize=8,\n",
    "                fontweight=\"bold\"\n",
    "            )\n",
    "            n = data.shape[0]  # assuming square matrix            \n",
    "            axes[0].plot(\n",
    "                [-0.5, n - 0.5],\n",
    "                [-0.5, n - 0.5],\n",
    "                color=\"black\",\n",
    "                linewidth=2,\n",
    "                linestyle=\"--\"   # optional\n",
    "            )   \n",
    "\n",
    "# ---------- Second matrix ----------\n",
    "data1 = M1.values * 902\n",
    "im1 = axes[1].imshow(data1, aspect=\"equal\", vmin=0, vmax=vmax)\n",
    "\n",
    "axes[1].set_xticks(range(12))\n",
    "axes[1].set_xticklabels(range(1, 13))\n",
    "axes[1].set_yticks(range(12))\n",
    "axes[1].set_yticklabels(range(1, 13))\n",
    "axes[1].set_xlabel(\"To period\")\n",
    "axes[1].set_title(\"Robust v Deterministic\")\n",
    "\n",
    "axes[1].set_xlim(-0.5, 11.5)\n",
    "axes[1].set_ylim(11.5, -0.5)\n",
    "\n",
    "norm = im1.norm\n",
    "cmap = im1.cmap\n",
    "\n",
    "for i in range(data1.shape[0]):\n",
    "    for j in range(data1.shape[1]):\n",
    "        value = data1[i, j]\n",
    "        if not np.isnan(value) and value > 0:\n",
    "            rgba = cmap(norm(value))\n",
    "            brightness = 0.299*rgba[0] + 0.587*rgba[1] + 0.114*rgba[2]\n",
    "            text_color = \"black\" if brightness > 0.5 else \"white\"\n",
    "\n",
    "            axes[1].text(\n",
    "                j, i,\n",
    "                f\"{value:,.0f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=text_color,\n",
    "                fontsize=8,\n",
    "                fontweight=\"bold\"\n",
    "            )\n",
    "            n = data.shape[0]  # assuming square matrix            \n",
    "            axes[1].plot(\n",
    "                [-0.5, n - 0.5],\n",
    "                [-0.5, n - 0.5],\n",
    "                color=\"black\",\n",
    "                linewidth=2,\n",
    "                linestyle=\"--\"   # optional\n",
    "            )   \n",
    "\n",
    "# ---------- Shared colorbar ----------\n",
    "cbar = fig.colorbar(\n",
    "    im0,\n",
    "    ax=axes,\n",
    "    fraction=0.021,   # keeps colorbar proportional\n",
    "    pad=0.04,\n",
    "    label=\"Area with a change of harvesting (ha)\"\n",
    ")\n",
    "\n",
    "\n",
    "plt.savefig(\n",
    "    path_out + \"BALANCE\" + TYPE + \"_\" + differe + \".png\",\n",
    "    dpi=300\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not needed for MAX NPV:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#Constructing a version of Figure 6 -- only used for the case when periodic income is important\n",
    "target = 600000*5\n",
    "periods = range(1, 13)\n",
    "\n",
    "per_ha = target\n",
    "# compute expected shortfall\n",
    "shortfall = {\n",
    "    \"Deterministic\": [],\n",
    "    \"Robust\": [],\n",
    "    \"Stochastic\": [],\n",
    "}\n",
    "\n",
    "for l in periods:\n",
    "    shortfall[\"Deterministic\"].append(\n",
    "        100 * np.mean(np.maximum(0, target - np.array(ef_det[l]))/per_ha)\n",
    "    )\n",
    "    shortfall[\"Robust\"].append(\n",
    "        100 * np.mean(np.maximum(0, target - np.array(ef_rob[l]))/per_ha)\n",
    "    )\n",
    "    shortfall[\"Stochastic\"].append(\n",
    "        100 * np.mean(np.maximum(0, target - np.array(ef_sto[l]))/per_ha)\n",
    "    )\n",
    "\n",
    "df_shortfall = pd.DataFrame(shortfall, index=[f\"P{l}\" for l in periods])\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.heatmap(\n",
    "    df_shortfall,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"Reds\",\n",
    "    cbar_kws={\"label\": \"Expected shortfall (%)\"}\n",
    ")\n",
    "\n",
    "\n",
    "#plt.xlabel(\"Formulation\")\n",
    "plt.ylabel(\"Period\")\n",
    "plt.title(\"Expected harvest shortfall relative to target (%)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    path_out+\"Periodic_expected_shortfall_\"+TYPE+\".png\",\n",
    "    bbox_inches=\"tight\"\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
